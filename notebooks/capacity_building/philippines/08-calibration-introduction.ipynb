{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e04f0b",
   "metadata": {},
   "source": [
    "# Introduction to calibration and uncertainty propagation\n",
    "Up until now we have been creating models that may accurately represent the local epidemic but (at best) only provide one possible epidemic profile that would be consistent with the observations. In this notebook, we extend this to obtain a range of parameter values and epidemic trajectories that would be consistent with the local observations, and thereby quantify the uncertainty in our simulations.\n",
    "\n",
    "In this notebook, we will learn how to use a Markov Chain Monte Carlo (MCMC) algorithm to calibrate an SIR model to epidemic data.\n",
    "That is, we will use a Bayesian sampling approach to estimate model parameters and to project the epidemic with uncertainty.\n",
    "\n",
    "We will implement the Metropolis algorithm which is one type of MCMC.\n",
    "\n",
    "Recommended pre-reading:\n",
    "- Wikipedia page on Metropolisâ€“Hastings algorithm [here](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm),\n",
    "- Some example implementations with discussion of common tuning issues [here](https://jellis18.github.io/post/2018-01-02-mcmc-part1/).\n",
    "\n",
    "\n",
    "And also, a great interactive demo of multiple Bayesian sampling algorithms [here](https://chi-feng.github.io/mcmc-demo/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install the required packages if running in Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  %pip install summerepi2\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !wget https://raw.githubusercontent.com/monash-emu/AuTuMN/master/notebooks/capacity_building/common_files/calibration_.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19523e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports, plotting option and constant definition\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from summer2 import CompartmentalModel\n",
    "from summer2.parameters import Parameter\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5f058",
   "metadata": {},
   "source": [
    "## Create some dummy data we want our model to fit to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"active_cases\":\n",
    "{\n",
    "    60.: 3000.,\n",
    "    80.: 8500.,\n",
    "    100.: 21000.,\n",
    "    120.: 40000.,\n",
    "    140.: 44000.,\n",
    "    160.: 30000.,\n",
    "    180.: 16000.,\n",
    "    200.: 7000.,\n",
    "}}\n",
    ")\n",
    "data['active_cases'].plot(kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90cd4d",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Define a simple SIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sir_model(model_config: dict) -> CompartmentalModel:\n",
    "    \"\"\"\n",
    "    Create a compartmental model, with the minimal compartmental structure needed to run and produce some sort of \n",
    "    meaningful outputs.\n",
    "    \n",
    "    Args:\n",
    "        model_config: Fixed values that determine structural and numerical properties of the model\n",
    "    Returns:\n",
    "        A compartmental model currently without stratification applied\n",
    "    \"\"\"\n",
    "\n",
    "    model = CompartmentalModel(\n",
    "        times=(model_config[\"start_time\"], model_config[\"end_time\"]),\n",
    "        compartments=[\"S\", \"I\", \"R\"],\n",
    "        infectious_compartments=[\"I\"],\n",
    "    )\n",
    "\n",
    "    infectious_seed = model_config[\"infectious_seed\"]\n",
    "    initial_population = model_config[\"initial_population\"]\n",
    "    assert initial_population >= infectious_seed, \"Initial population size must be greater than infectious seed\"\n",
    "\n",
    "    model.set_initial_population(\n",
    "        distribution=\n",
    "        {\n",
    "            \"S\": initial_population - infectious_seed, \n",
    "            \"I\": infectious_seed\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Set up flows with summer2 Parameter objects - these are placeholders\n",
    "    # whose actual values will be looked up in a dictionary when we run the model\n",
    "    \n",
    "    # Susceptible people can get infected\n",
    "    model.add_infection_frequency_flow(\n",
    "        name=\"infection\", \n",
    "        contact_rate=Parameter(\"contact_rate\"), \n",
    "        source=\"S\", \n",
    "        dest=\"I\",\n",
    "    )\n",
    "    \n",
    "    # Note that you can perform arithmetic and other transforms on Parameter objects - \n",
    "    # their final values will be computed later\n",
    "    \n",
    "    # Infectious people recover\n",
    "    model.add_transition_flow(\n",
    "        name=\"recovery\",\n",
    "        fractional_rate= 1. / Parameter(\"infection_duration\"),\n",
    "        source=\"I\",\n",
    "        dest=\"R\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fa5be",
   "metadata": {},
   "source": [
    "## Run the model with some example parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097609b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    # Fixed configuration options that define the structure and behaviour of the model\n",
    "    \"initial_population\": 1.e6,\n",
    "    \"infectious_seed\": 100.,\n",
    "    \"start_time\": 0,\n",
    "    \"end_time\": 365,\n",
    "}\n",
    "# Get an SIR model object\n",
    "sir_model = build_sir_model(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of free parameters that we will use in calibration - ie those that we\n",
    "# declared as Parameter objects when building the model\n",
    "parameters = {\n",
    "    \"contact_rate\": 0.3,\n",
    "    \"infection_duration\": 9.0\n",
    "}\n",
    "\n",
    "# Run the model with the dummy parameter values\n",
    "sir_model.run(parameters)\n",
    "\n",
    "# Plot the model outputs against the data\n",
    "output_df = pd.DataFrame({\n",
    "    \"modelled\": sir_model.get_outputs_df()[\"I\"],\n",
    "    \"observed\": data.active_cases\n",
    "})\n",
    "output_df.plot(kind='scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5851e9",
   "metadata": {},
   "source": [
    "# Calibration specifications\n",
    "## Posterior distribution\n",
    "The main objective of our calibration is to estimate the **posterior distribution** of the calibrated parameters. This is the probability distribution of the parameters that are able to describe our observations, given some prior knowledge about these parameters and a mathematical model. In other words, this tells us \"What values the parameters should take such that our model is able to capture the data, and given any prior information we had about the parameters before even running the model\".\n",
    "\n",
    "### Overview of the posterior computation\n",
    "![title](../common_files/calibration_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49006091",
   "metadata": {},
   "source": [
    "## Bayes Theorem used to decide parameter acceptance\n",
    "\n",
    "\n",
    "The posterior probability of a parameter set $\\theta$ associated with the data $y$ is denoted $P(\\theta | y)$.\n",
    "\n",
    "Let's write the Bayes Therorem for reference:\n",
    "$$P(\\theta | y) = \\frac{P(y | \\theta) \\times P(\\theta)}{P(y)} \\quad.$$\n",
    "\n",
    "Within the MCMC loop, we are only interested in the acceptance ratio that defines the probability of acceptance of a newly proposed parameter set $\\theta '$, when the last accepted parameter set was $\\theta$. This is the ratio of the posterior probabilities between $\\theta '$ and $\\theta$:\n",
    "$$H := \\frac{P(\\theta ' | y)}{P(\\theta | y)} = \\frac{\\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y)}}{\\frac{P(y | \\theta) \\times P(\\theta)}{P(y)}} = \\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y | \\theta) \\times P(\\theta)} \\quad .$$\n",
    "\n",
    "If $H \\geq 1$, we accept the proposed parameter set $\\theta'$. Otherwise, the proposed parameter set $\\theta'$ is accepted with probability $H$. \n",
    "\n",
    "Here we will define the fundamental aspects of our MCMC calibration:\n",
    "- Our prior knowledge about the calibrated parameters: $P(\\theta)$\n",
    "- The likelihood associated with our model. This is the probability of observing the data under a given model parameterisation: $P(y|\\theta)$\n",
    "\n",
    "... and some other technical aspects:\n",
    "- Intitial point from which the MCMC algorithm starts\n",
    "- The proposal function (or jumping process), defining how we move around in our parameter space. This is defined by $\\pi(\\theta' | \\theta)$ which is the probability of reaching the parameter set $\\theta'$, when starting from the parameter set $\\theta$.\n",
    "\n",
    "### Using log-transformed quantities\n",
    "\n",
    "The prior and likelihood quantities are often extremely small numbers in practice, which may make computation difficult due to computer precision limits. To avoid issues related to rounding, the probabilities are usually transformed using the logarithm function before calculation of the acceptance ratio.\n",
    "\n",
    "$$H=\\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y | \\theta) \\times P(\\theta)} \\\\\n",
    "= exp\\Bigl( \\Bigl[\\ln\\bigl(P(y|\\theta')\\bigr) + \\ln\\bigl(P(\\theta')\\bigr) \\Bigr] - \\Bigl[\\ln\\bigl(P(y|\\theta)\\bigr) + \\ln\\bigl(P(\\theta)\\bigr) \\Bigr] \\Bigr) \\\\\n",
    "= exp( A(\\theta') - A(\\theta)) \\quad,\n",
    "$$\n",
    "where $A(\\theta):=\\ln\\bigl(P(y|\\theta)\\bigr) + \\ln\\bigl(P(\\theta)\\bigr) $ will be referred to as the acceptance quantity associated with parameter set $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f4cef",
   "metadata": {},
   "source": [
    "### Prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_priors(proposed_parameters: dict) -> float:\n",
    "    # Initialise the prior likelihood to 1\n",
    "    prior_log_proba = 0.\n",
    "\n",
    "    # Use a uniform prior on [0., 0.5] for the contact_rate \n",
    "    prior_log_proba += stats.uniform.logpdf(x=proposed_parameters['contact_rate'], loc=0, scale=0.5)\n",
    "\n",
    "    # Use a normal prior for the infection duration, with mean=7 days and sd=.5\n",
    "    prior_log_proba += stats.norm.logpdf(x=proposed_parameters['infection_duration'], loc=7, scale=.5)\n",
    "\n",
    "    return prior_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2eec3",
   "metadata": {},
   "source": [
    "### Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_likelihood(model, proposed_parameters: dict) -> float:\n",
    "\n",
    "    # build and run the model with the selected parameters\n",
    "    parameter_set = dict(proposed_parameters)\n",
    "    model.run(parameter_set)\n",
    "    modelled_active = model.get_outputs_df()['I']\n",
    "\n",
    "    # calculate the log-likelihood associated with the model run\n",
    "    log_likelihood = 0.\n",
    "    for data_time, data_value in data['active_cases'].iteritems():\n",
    "        modelled_value = modelled_active.loc[data_time]\n",
    "        # use a normal likelihood with sd=100, centered on the model estimate\n",
    "        log_likelihood += stats.norm.logpdf(x=data_value, loc=modelled_value, scale=10000.)\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d5e78",
   "metadata": {},
   "source": [
    "### Acceptance quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ec261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acceptance_quantity(model: CompartmentalModel, proposed_parameters: dict) -> float:\n",
    "\n",
    "    log_prior = evaluate_log_priors(proposed_parameters)\n",
    "    log_likelihood = evaluate_log_likelihood(model, proposed_parameters)\n",
    "    acceptance_quantity = log_prior + log_likelihood\n",
    "    \n",
    "    return acceptance_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddc55d",
   "metadata": {},
   "source": [
    "### Proposal (jumping) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_parameter_set(previous_parameters: dict, jumping_sds: dict) -> dict:\n",
    "    \n",
    "    proposed_parameters = {}\n",
    "    for param_name in [\"contact_rate\", \"infection_duration\"]:\n",
    "        proposed_parameters[param_name] = stats.norm.rvs(loc=previous_parameters[param_name], scale=jumping_sds[param_name])\n",
    "\n",
    "    return proposed_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d6517",
   "metadata": {},
   "source": [
    "## The Metropolis algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_metropolis(model: CompartmentalModel, n_iter: int, initial_parameters: dict, jumping_sds:dict) -> dict:\n",
    "   \n",
    "    mcmc_record = pd.DataFrame(\n",
    "        index=range(n_iter), \n",
    "        columns=[\"contact_rate\", \"infection_duration\", \"acceptance_quantity\", \"changed_position\"],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "    current_parameters = initial_parameters\n",
    "    current_acceptance_quantity = evaluate_acceptance_quantity(model, current_parameters) \n",
    "\n",
    "    for i_run in range(n_iter):\n",
    "        # Print a periodic update so we know things are running...\n",
    "        \n",
    "        if (i_run % 1000) == 0:\n",
    "            print(f\"Running iter {i_run} of {n_iter} iterations\")\n",
    "        \n",
    "        # Propose a new parameter set and evaluate its acceptance quantity\n",
    "        proposed_parameters = propose_parameter_set(current_parameters, jumping_sds)\n",
    "        proposed_acceptance_quantity = evaluate_acceptance_quantity(model, proposed_parameters) \n",
    "\n",
    "        # Decide whether to accept the proposed parameters or not\n",
    "        if proposed_acceptance_quantity >= current_acceptance_quantity:\n",
    "            accept = 1\n",
    "        else:\n",
    "            accept_proba = np.exp(proposed_acceptance_quantity - current_acceptance_quantity)\n",
    "            accept = stats.binom.rvs(1, accept_proba)  # flip a coin\n",
    "        \n",
    "        # Update the MCMC sampler in case of acceptance\n",
    "        if accept == 1:\n",
    "            current_parameters = proposed_parameters\n",
    "            current_acceptance_quantity = proposed_acceptance_quantity\n",
    "\n",
    "        # Record the current state\n",
    "        mcmc_record.loc[i_run] = {\n",
    "            \"contact_rate\": current_parameters['contact_rate'], \n",
    "            \"infection_duration\": current_parameters['infection_duration'], \n",
    "            \"acceptance_quantity\": float(current_acceptance_quantity),\n",
    "            \"changed_position\": accept\n",
    "        }\n",
    "    \n",
    "    return mcmc_record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a90f7a",
   "metadata": {},
   "source": [
    "# Let's calibrate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d4c9",
   "metadata": {},
   "source": [
    "## Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b79a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 5000\n",
    "initial_parameters = {\n",
    "    \"contact_rate\": 0.15,\n",
    "    \"infection_duration\": 10.,\n",
    "}\n",
    "\n",
    "jumping_sds = {\n",
    "    \"contact_rate\": .01,\n",
    "    \"infection_duration\": .5\n",
    "}\n",
    "\n",
    "mcmc_output = sample_with_metropolis(sir_model, n_iterations, initial_parameters, jumping_sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8992",
   "metadata": {},
   "source": [
    "## Explore the MCMC outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f611dad",
   "metadata": {},
   "source": [
    "### Acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accepted = mcmc_output['changed_position'].sum()\n",
    "acceptance_perc = 100. * n_accepted / n_iterations\n",
    "print(f\"Our MCMC's acceptance rate is: {round(acceptance_perc,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf115b97",
   "metadata": {},
   "source": [
    "### Progression of the acceptance quantity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output['acceptance_quantity'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4848601",
   "metadata": {},
   "source": [
    "### Parameter traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output[\"contact_rate\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output[\"infection_duration\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d38c06",
   "metadata": {},
   "source": [
    "### Burn-in\n",
    "We want to discard the parameter sets sampled before convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in = round(n_iterations / 2.)\n",
    "post_burn_in_mcmc_output = mcmc_output[burn_in:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94028318",
   "metadata": {},
   "source": [
    "### Posterior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output[\"contact_rate\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f88941",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output[\"infection_duration\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df80940",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output.plot.scatter(x=\"contact_rate\", y=\"infection_duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e1a86",
   "metadata": {},
   "source": [
    "### The best prameter set (with regards to the posterior likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_id = post_burn_in_mcmc_output['acceptance_quantity'].idxmax()\n",
    "best_parameters = {\n",
    "    \"contact_rate\": post_burn_in_mcmc_output.loc[best_run_id][\"contact_rate\"],\n",
    "    \"infection_duration\": post_burn_in_mcmc_output.loc[best_run_id][\"infection_duration\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sir_model.run(best_parameters)\n",
    "# Plot the model outputs against the data\n",
    "output_df = pd.DataFrame({\n",
    "    \"modelled\": sir_model.get_outputs_df()[\"I\"],\n",
    "    \"observed\": data.active_cases\n",
    "})\n",
    "output_df.plot(kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960197b",
   "metadata": {},
   "source": [
    "### Plot 100 sampled model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(100, n_iterations - burn_in)\n",
    "sampled_df = post_burn_in_mcmc_output.sample(n_samples)\n",
    "\n",
    "sampled_output_df = pd.DataFrame(index=sir_model.get_outputs_df().index)\n",
    "for i_run in sampled_df.index:\n",
    "    selected_parameters = {\n",
    "        \"contact_rate\": post_burn_in_mcmc_output.loc[i_run][\"contact_rate\"],\n",
    "        \"infection_duration\": post_burn_in_mcmc_output.loc[i_run][\"infection_duration\"],\n",
    "    }\n",
    "    sir_model.run(selected_parameters)\n",
    "    sampled_output_df[i_run] = sir_model.get_outputs_df()[\"I\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "\n",
    "fig = sampled_output_df.plot(legend=None, figsize=(15,8))\n",
    "fig.plot(data.index, data[\"active_cases\"], marker=\".\",color='black', lw=0,ms=10)\n",
    "\n",
    "# Restore this for other plots\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff20bf",
   "metadata": {},
   "source": [
    "## Some further considerations\n",
    "We have presented a very simple implementation of an MCMC-based calibration. In practice, there may be other aspects to consider, including:\n",
    "\n",
    "- Use of other MCMC algorithms\n",
    "\n",
    "Here we have implemented a \"simple\" Metropolis-Hastings algorithm, which is the simplest version of MCMCs. However, there exist other types of MCMCs including Gibbs sampling and the Hamiltonian Monte-Carlo. There are also other Bayesian sampling methods that don't verify the Markov property (i.e. not MCMC) but can be used for the same purpose. This includes adaptive Metropolis samplers such as the Haario algorithm.\n",
    "\n",
    "- Use of multiple MCMC chains\n",
    "\n",
    "We have only implemented a single MCMC chain that explores the parameter set and samples posterior estimates. In practice, it is common to use multiple chains that can be run in parallel to generate more samples in the same period of time. Samples from the different chains are then combined and we can perform statistical tests to check for convergence and consistency between the chains (e.g. R-hat statistic).\n",
    "\n",
    "- Non-symmetric proposal function\n",
    "\n",
    "We have used a symmetric proposal (jumping) function in this example. This means that $\\pi(\\theta'|\\theta) = \\pi(\\theta|\\theta')$. If the proposal function is not symmetric, we should adjust the acceptance ratio as follows:\n",
    "$$ H= \\frac{P(y | \\theta ') \\times P(\\theta ') \\times \\pi(\\theta|\\theta') }{P(y | \\theta) \\times P(\\theta) \\times \\pi(\\theta' |\\theta)} \\quad .$$\n",
    "\n",
    "- Parameter transformation\n",
    "\n",
    "When parameter supports are bounded (e.g. finite interval), we often transform the parameters into quantities that are unbounded to make sampling easier. For example, with the transformed parameter space, we don't have to worry about having a proposal function defined on a bounded support. These transformations imply some more adjustments to the acceptance ratio that are not discussed here.\n",
    "\n",
    "- Thinning\n",
    "\n",
    "The samples generated by some MCMC algorithms (e.g. Metropolis-Hastings) are often highly auto-correlated. This is due to the iterative way in which the samples are generated. To address this issue we often apply thinning after generating the samples. That is, we only retain every n-th sampled paramerer sets.\n",
    "\n",
    "- Algorithm tuning...\n",
    "\n",
    "This is probably the most challenging aspect of the Metropolis-Hastings sampler. This is about finding an adequate proposal (jumping) function that will ensure an exhaustive and efficient exploration of the parameter space. If transitions (or jumps) are too big, we will rarely accept the proposed parameters because they would be outside the high-density regions. If transitions are too small, we may not explore the parameter space comprehensively because we may always stay in the same regions. There is no pre-defined rule about how to define a \"good\" proposal function, but we often want to achieve an acceptance rate of about 10-40%.\n",
    "\n",
    "## Now the good news...\n",
    "There are multiple libraries that handle Bayesian sampling with MCMC algorithms already implemented. They also have self-tuning functionalities and other features (e.g. automatic parameter transformation) that address the issues listed above. Our next session will introduce one of these libraries: numpyro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7afc08b952f75bca94590012dd49682c815a0fa68720c270ce23d7ae27bf110a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
