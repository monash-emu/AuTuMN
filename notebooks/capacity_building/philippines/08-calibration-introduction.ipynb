{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e04f0b",
   "metadata": {},
   "source": [
    "# Introduction to calibration and uncertainty propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4870297",
   "metadata": {},
   "source": [
    "## Problem to solve\n",
    "\n",
    "Explain the problem here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install the required packages if running in Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  %pip install summerepi\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19523e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports, plotting option and constant definition\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from summer import CompartmentalModel\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5f058",
   "metadata": {},
   "source": [
    "## Create some dummy data we want our model to fit to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"active_cases\":\n",
    "{\n",
    "    60.: 3000.,\n",
    "    80.: 8500.,\n",
    "    100.: 21000.,\n",
    "    120.: 40000.,\n",
    "    140.: 44000.,\n",
    "    160.: 30000.,\n",
    "    180.: 16000.,\n",
    "    200.: 7000.,\n",
    "}}\n",
    ")\n",
    "data['active_cases'].plot(kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90cd4d",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Define a simple SIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sir_model(parameters: dict) -> CompartmentalModel:\n",
    "    \"\"\"\n",
    "    Create a compartmental model, with the minimal compartmental structure needed to run and produce some sort of \n",
    "    meaningful outputs.\n",
    "    \n",
    "    Args:\n",
    "        parameters: Flow parameters\n",
    "    Returns:\n",
    "        A compartmental model currently without stratification applied\n",
    "    \"\"\"\n",
    "\n",
    "    model = CompartmentalModel(\n",
    "        times=(parameters[\"start_time\"], parameters[\"end_time\"]),\n",
    "        compartments=[\"S\", \"I\", \"R\"],\n",
    "        infectious_compartments=[\"I\"],\n",
    "    )\n",
    "\n",
    "    infectious_seed = parameters[\"infectious_seed\"]\n",
    "    initial_population = parameters[\"initial_population\"]\n",
    "    assert initial_population >= infectious_seed, \"Initial population size must be greater than infectious seed\"\n",
    "\n",
    "    model.set_initial_population(\n",
    "        distribution=\n",
    "        {\n",
    "            \"S\": initial_population - infectious_seed, \n",
    "            \"I\": infectious_seed\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Susceptible people can get infected\n",
    "    model.add_infection_frequency_flow(\n",
    "        name=\"infection\", \n",
    "        contact_rate=parameters[\"contact_rate\"], \n",
    "        source=\"S\", \n",
    "        dest=\"I\",\n",
    "    )\n",
    "    \n",
    "    # Infectious people recover\n",
    "    model.add_transition_flow(\n",
    "        name=\"recovery\",\n",
    "        fractional_rate= 1. / parameters[\"infection_duration\"],\n",
    "        source=\"I\",\n",
    "        dest=\"R\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fa5be",
   "metadata": {},
   "source": [
    "## Run the model with some example parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097609b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_parameters = {\n",
    "    \"initial_population\": 1.e6,\n",
    "    \"infectious_seed\": 100.,\n",
    "    \"start_time\": 0,\n",
    "    \"end_time\": 365,\n",
    "    \n",
    "    \"contact_rate\": 0.3,\n",
    "    \"infection_duration\": 7.,\n",
    "}\n",
    "\n",
    "# Get an SIR model object\n",
    "model = build_sir_model(base_parameters)\n",
    "\n",
    "# Run the model\n",
    "model.run()\n",
    "\n",
    "# Plot the model outputs against the data\n",
    "output_df = pd.DataFrame({\n",
    "    \"modelled\": model.get_outputs_df()[\"I\"],\n",
    "    \"observed\": data.active_cases\n",
    "})\n",
    "output_df.plot(kind='scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5851e9",
   "metadata": {},
   "source": [
    "## Calibration specifications\n",
    "The main objective of our calibration is to estimate the **posterior distribution** of the calibrated parameters. This is the probability distribution of the parameters that are able to describe our observations, given some prior knowledge about these parameters and a mathematical model. In other words, this tells us \"What values the parameters should take such that our model is able to capture the data, and given any prior information we had about the parameters before even running the model\".\n",
    "\n",
    "The posterior probability of a parameter set $\\theta$ associated with the data $y$ is denoted $P(\\theta | y)$.\n",
    "\n",
    "Let's write the Bayes Therorem for reference:\n",
    "$$P(\\theta | y) = \\frac{P(y | \\theta) \\times P(\\theta)}{P(y)} \\quad.$$\n",
    "\n",
    "Within the MCMC loop, we are only interested in the acceptance ratio that defines the probability of acceptance of a newly proposed parameter set $\\theta '$, when the last accepted parameter set was $\\theta$. This is the ratio of the posterior probabilities between $\\theta '$ and $\\theta$:\n",
    "$$H := \\frac{P(\\theta ' | y)}{P(\\theta | y)} = \\frac{\\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y)}}{\\frac{P(y | \\theta) \\times P(\\theta)}{P(y)}} = \\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y | \\theta) \\times P(\\theta)} \\quad .$$\n",
    "\n",
    "Here we will define the fundamental aspects of our MCMC calibration:\n",
    "- Our prior knowledge about the calibrated parameters: $P(\\theta)$\n",
    "- The likelihood associated with our model. This is the probability of observing the data under a given model parameterisation: $P(y|\\theta)$\n",
    "\n",
    "... and some other technical aspects:\n",
    "- Intitial point from which the MCMC algorithm starts\n",
    "- The proposal function (or jumping process), defining how we move around in our parameter space. This is defined by $\\pi(\\theta' | \\theta)$ which is the probability of reaching the parameter set $\\theta'$, when starting from the parameter set $\\theta$.\n",
    "\n",
    "### Using log-transformed quantities\n",
    "\n",
    "The prior and likelihood quantities are often extremely small numbers in practice, which may make computation difficult due to computer precision limits. To avoid issues related to rounding, the probabilities are usually transformed using the logarithm function before calculation of the acceptance ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f4cef",
   "metadata": {},
   "source": [
    "### Prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_priors(proposed_parameters: dict) -> float:\n",
    "    # Initialise the prior likelihood to 1\n",
    "    prior_log_proba = 0.\n",
    "\n",
    "    # Use a uniform prior on [0., 0.5] for the contact_rate \n",
    "    prior_log_proba += stats.uniform.logpdf(x=proposed_parameters['contact_rate'], loc=0, scale=0.5)\n",
    "\n",
    "    # Use a normal prior for the infection duration, with mean=7 days and sd=.5\n",
    "    prior_log_proba += stats.norm.logpdf(x=proposed_parameters['infection_duration'], loc=7, scale=.5)\n",
    "\n",
    "    return prior_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2eec3",
   "metadata": {},
   "source": [
    "### Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_likelihood(proposed_parameters: dict) -> float:\n",
    "\n",
    "    # build and run the model with the selected parameters\n",
    "    parameter_set = dict(base_parameters, **proposed_parameters)\n",
    "    model = build_sir_model(parameter_set)\n",
    "    model.run()\n",
    "    modelled_active = model.get_outputs_df()['I']\n",
    "\n",
    "    # calculate the log-likelihood associated with the model run\n",
    "    log_likelihood = 0.\n",
    "    for data_time, data_value in data['active_cases'].iteritems():\n",
    "        modelled_value = modelled_active.loc[data_time]\n",
    "        # use a normal likelihood with sd=100, centered on the model estimate\n",
    "        log_likelihood += stats.norm.logpdf(x=data_value, loc=modelled_value, scale=.1)\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddc55d",
   "metadata": {},
   "source": [
    "### Proposal (jumping) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jumping_sds = {\n",
    "    \"contact_rate\": .1,\n",
    "    \"infection_duration\": .5\n",
    "}\n",
    "\n",
    "def propose_parameter_set(previous_parameters: dict, jumping_sds:dict) -> dict:\n",
    "    \n",
    "    proposed_parameters = {}\n",
    "    for param_name in [\"contact_rate\", \"infection_duration\"]:\n",
    "        proposed_parameters[param_name] = stats.norm.rvs(loc=previous_parameters[param_name], scale=jumping_sds[param_name])\n",
    "\n",
    "    return proposed_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546dcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_set_1 = {\n",
    "    \"contact_rate\": .4,\n",
    "    \"infection_duration\": 6.\n",
    "}\n",
    "\n",
    "param_set_2 = {\n",
    "    \"contact_rate\": .4,\n",
    "    \"infection_duration\": 12.\n",
    "}\n",
    "\n",
    "# print(propose_parameter_set(param_set_2, jumping_sds))\n",
    "\n",
    "#print(evaluate_log_likelihood(param_set_2))\n",
    "\n",
    "\n",
    "#print(evaluate_priors(param_set_1))\n",
    "#print(evaluate_priors(param_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff20bf",
   "metadata": {},
   "source": [
    "## Some further considerations\n",
    "We have presented a very simple implementation of an MCMC-based calibration. In practice, there may be other aspects to consider, including:\n",
    "\n",
    "- Use of other MCMC algorithms\n",
    "\n",
    "Here we have implemented a \"simple\" Metropolis-Hastings algorithm, which is the simplest version of MCMCs. However, there exist other types of MCMCs including Gibbs sampling and the Hamiltonian Monte-Carlo. There are also other Bayesian sampling methods that don't verify the Markov property (i.e. not MCMC) but can be used for the same purpose. This includes adaptive Metropolis samplers such as the Haario algorithm.\n",
    "\n",
    "- Use of multiple MCMC chains\n",
    "\n",
    "We have only implemented a single MCMC chain that explores the parameter set and samples posterior estimates. In practice, it is common to use multiple chains that can be run in parallel to generate more samples in the same period of time. Samples from the different chains are then combined and we can perform statistical tests to check for convergence and consistency between the chains (e.g. R-hat statistic).\n",
    "\n",
    "- Non-symmetric proposal function\n",
    "\n",
    "We have used a symmetric proposal (jumping) function in this example. This means that $\\pi(\\theta'|\\theta) = \\pi(\\theta|\\theta')$. If the proposal function is not symetric, we should adjust the acceptance ratio as follows:\n",
    "$$ H= \\frac{P(y | \\theta ') \\times P(\\theta ') \\times \\pi(\\theta|\\theta') }{P(y | \\theta) \\times P(\\theta) \\times \\pi(\\theta' |\\theta)} \\quad .$$\n",
    "\n",
    "- Parameter transformation\n",
    "\n",
    "When parameter supports are bounded (e.g. finite interval), we often transform the parameters into quantities that are unbounded to make sampling easier. For example, with the transformed parameter space, we don't have to worry about having a proposal function defined on a bounded support. These transformations imply some more adjustments to the acceptance ratio that are not discussed here.\n",
    "\n",
    "- Thinning\n",
    "\n",
    "The samples generated by some MCMC algorithms (e.g. Metropolis-Hastings) are often highly auto-correlated. This is due to the iterative way in which the samples are generated. To address this issue we often apply thinning after generating the samples. That is, we only retain every n-th sampled paramerer sets.\n",
    "\n",
    "- Algorithm tuning...\n",
    "\n",
    "This is probably the most challenging aspect of the Metropolis-Hastings sampler. This is about finding an adequate proposal (jumping) function that will ensure an exhaustive and efficient exploration of the parameter space. If transitions (or jumps) are too big, we will rarely accept the proposed parameters because they would be outside the high-density regions. If transitions are too small, we may not explore the parameter space comprehensively because we may always stay in the same regions. There is no pre-defined rule about how to define a \"good\" proposal function, but we often want to achieve an acceptance rare of about 10-40%.\n",
    "\n",
    "## Now the good news...\n",
    "There are multiple libraries that handle Bayesian sampling with MCMC algorithms already implemented. They also have self-tuning functionalities and other features (e.g. automatic parameter transformation) that address the issues listed above. Our next session will introduce one of these libraries: numpyro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306314b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('autumn310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7afc08b952f75bca94590012dd49682c815a0fa68720c270ce23d7ae27bf110a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
